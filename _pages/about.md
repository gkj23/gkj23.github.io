---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


## ğŸ“ Academic Background

I am currently an undergraduate student in the **Department of Computer Science and Technology** at **Tsinghua University**, a member of the Class of 2023.

As of the end of my sophomore year, my academic standing is **GPA 3.90** (ranked **23/171** in the cohort). Notably, **all core Computer Science courses and fundamental Mathematics courses were completed with a 4.0**.

Prior to this, I completed my high school education in Jinan, Shandong Province, where I achieved a score of **704** on the National College Entrance Examination, ranking among the **Top 10** students in the province.

## ğŸ‘©â€ğŸ’» Research Experience

During my undergraduate studies, I have been actively involved in research internships under the guidance of **Prof. Song-Hai Zhang**, **Prof. Xiao-Lin Hu**, and **Prof. Hao Zhao**.

### Key Achievements
* **SRT Project (Prof. Zhang):** My individual work on the Student Research Training (SRT) project under Prof. Zhang received the **A+ grade (Highest)**, demonstrating exceptional research output.
* **Research Grant (Prof. Hu):** During my internship with Prof. Hu, I successfully initiated and secured a **20,000 RMB** grant through the competitive university-level "Xuetui Plan" (Student Research Promotion Program).



## ğŸ”¬ Research Interests

Broadly speaking, my research centers on the **perception, understanding, and generation of visual and auditory modalities**. My ultimate goal is to construct a **highly realistic and interactive audiovisual world**, serving as a foundation to achieve **Spatial Intelligence** capable of robust perception, understanding, and reasoning.

Specifically, I focus (or plan to focus) on the following areas:

### 1. 3D Vision
* **3D Representation Learning:** Considering the current fragmented landscape of 3D representations (e.g., Voxels, NeRF, Gaussian Splatting), I aim to explore and define a **unified 3D representation paradigm**.
* **3D Scene Generation & Reconstruction:** Generating interactive and Physically Realistic 3D scenes.

### 2. Spatial Audio
*(For a comprehensive overview of this field, refer to the survey: [ASAudio: A Survey of Advanced Spatial Audio Research](https://arxiv.org/abs/2508.10924))*

* **Acoustic Field Reconstruction & Generation:** Inspired by works such as [NeRAF](https://amandinebtto.github.io/NeRAF/) and [AV-DAR](https://humathe.github.io/avdar/), I plan to develop a **feedforward Audio-Visual Gaussian Splatting (AV-GS) framework**, which supports multiple sound sources and generalizes across scenes without requiring per-scene optimization. 
* **Unified Spatial Audio Generation:** Developing a unified framework capable of generating spatial audio from diverse inputs, including text, egocentric video, 360Â° video, and audio.
* **Spatial Audio Perception & Reasoning:** Investigating how **Multimodal LLMs (MLLMs)** perform reasoning using spatial audio, and how Embodied AI frameworks (e.g., **Vision-Language-Action (VLA)** models) utilize spatial audio for decision-making.

### 3. Multimodal Large Language Models (MLLM)
* **Robustness in Audiovisual Contexts:** Benchmarking and enhancing MLLM robustness under challenging conditions, such as noisy environments, multi-speaker audio, or extremely low-quality visual inputs.
* **Enhancing Spatial Intelligence:** Improving performance on downstream spatial tasks, including but not limited to:
    * **Vision-and-Language Navigation (VLN)**
    * **3D Object Detection & Grounding**
    * **Spatial Question Answering (Spatial QA)**
* **MLLM Paradigm Research:** Critically examining current multimodal alignment paradigms (e.g., designing specific discrete tokenizers or continuous encoders for each modality)ï¼Œ I aim to investigate whether this is the path to true multimodal intelligence or merely an expedient, short-sighted solution for simplicity.


## ğŸ“š Publications / Works

* **ã€Šå™äº‹å·¥åŠï¼šäº¤äº’å¼å™äº‹åœºæ™¯æ„å»ºã€‹ (Narrative Workshop: Interactive Narrative Scene Construction)** (Authors: Hanxi Zhu, \textbf{Kejun Gao}, et al.). *Chinagraph 2024 **Best Paper Award**; Submitted to Chinese Journal of Computers (CCF-A), 2025.* [Link](https://xueshu.baidu.com/ndscholar/browse/detail?paperid=1p7j0gh0nx7m0gu0sj0x0j306s487904)
  > **Highlights:** æå‡ºäº†ä¸€ç§åŸºäº LLM çš„å™äº‹åœºæ™¯å¸ƒå±€ (Layout) ä¼˜åŒ–ç­–ç•¥ã€‚è· Chinagraph æœ€ä½³è®ºæ–‡å¥–ï¼ŒCNKI ä¸‹è½½é‡ 300+ã€‚

* **ViewSeeker: Locating Camera via Monocular RGB Image with MaskXY Derivatives** (Authors: Hanxi Zhu, \textbf{Kejun Gao}, et al.). *Under Review at IEEE Transactions on Visualization and Computer Graphics (TVCG, CCF-A).*
  > **Highlights:** è®¾è®¡äº†åŸºäºæ£€æµ‹åˆ†å‰²æ¨¡å‹è¾“å‡ºè®¡ç®—çš„ MaskXY å¯¼æ•°ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºç›®æ ‡è§†è§’å¯¼èˆª (Navigation) çš„å¯å¾®æ¸²æŸ“æ¡†æ¶ã€‚

* **Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention** (Authors: Kai Li*, \textbf{Kejun Gao}, et al.). *Under Review at ICLR 2026 (Scores befor Rebuttal: 6/6/6/4).* [Project Page](https://cslikai.cn/Dolphin)
  > **Highlights:** æå‡ºåŸºäºâ€œè¯­ä¹‰-é‡å»ºâ€åŒè·¯è§†è§‰ç¼–ç æ¡†æ¶çš„è½»é‡åŒ–å¤šæ¨¡æ€è¯­éŸ³åˆ†ç¦»æ¨¡å‹ã€‚åœ¨æ‰€æœ‰æ•°æ®é›†æŒ‡æ ‡ä¸Šè¶…è¶Š SOTAï¼Œå‚æ•°é‡å‡å°‘ **>50%**ï¼ŒMACs é™ä½ **>2.4Ã—**ï¼Œæ¨ç†é€Ÿåº¦æå‡ **>6Ã—** (GitHub 159 stars)ã€‚

* **LottieGPT: Tokenizing Vector Animation for Autoregressive Generation** (Authors: Junhao Chen*, \textbf{Kejun Gao}, et al.). *Under Review at CVPR 2026.*
  > **Highlights:** é¦–ä¸ªåŸºäº Lottie æ•°æ®æ ¼å¼çš„çŸ¢é‡åŠ¨ç”»ç”Ÿæˆæ¡†æ¶ã€‚åœ¨ SVG å•å›¾ç”Ÿæˆä¸Šè¶…è¶Š OmniSVG ç­‰ SOTAï¼Œåœ¨çŸ¢é‡è§†é¢‘ç”Ÿæˆä¸Šæ•ˆæœä¼˜äº Sora2ã€Kling ç­‰é—­æºæ¨¡å‹ã€‚


## ğŸ† Awards

* **2024-2025 Academic Year:** Awarded the **"KuanDe" Comprehensive Excellence Scholarship** (å®½å¾·ç»¼åˆä¼˜ç§€å¥–å­¦é‡‘).
* **2023-2024 Academic Year:** Awarded the **"Tsinghua Friend-Huawei" Comprehensive Excellence Scholarship** (æ¸…åä¹‹å‹-åä¸ºç»¼åˆä¼˜ç§€å¥–å­¦é‡‘).

